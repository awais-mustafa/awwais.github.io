<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Machine Learning - Visualized | Awais Mustafa</title><meta name=keywords content="ML,AI,Python,Visualization"><meta name=description content="A visual approach to understand machine learning"><meta name=author content="Awais"><link rel=canonical href=https://awwais.me/playbook/blog/machine-learning-visualized/><link crossorigin=anonymous href=/playbook/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/playbook/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://awwais.me/playbook/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://awwais.me/playbook/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://awwais.me/playbook/favicon-32x32.png><link rel=apple-touch-icon href=https://awwais.me/playbook/apple-touch-icon.png><link rel=mask-icon href=https://awwais.me/playbook/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Machine Learning - Visualized"><meta property="og:description" content="A visual approach to understand machine learning"><meta property="og:type" content="article"><meta property="og:url" content="https://awwais.me/playbook/blog/machine-learning-visualized/"><meta property="og:image" content="https://awwais.me/playbook/blog/machine-learning-visualized/cover.jpeg"><meta property="article:section" content="blog"><meta property="og:site_name" content="Awais"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://awwais.me/playbook/blog/machine-learning-visualized/cover.jpeg"><meta name=twitter:title content="Machine Learning - Visualized"><meta name=twitter:description content="A visual approach to understand machine learning"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Blogs","item":"https://awwais.me/playbook/blog/"},{"@type":"ListItem","position":3,"name":"Machine Learning - Visualized","item":"https://awwais.me/playbook/blog/machine-learning-visualized/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Machine Learning - Visualized","name":"Machine Learning - Visualized","description":"A visual approach to understand machine learning","keywords":["ML","AI","Python","Visualization"],"articleBody":"Introduction to machine learning In the traditional hard-coded approach, we program a computer to perform a certain task. We tell it exactly what to do when it receives a certain input. In mathematical terms, this is like saying that we write the f(x) such that when users feed the input x into f(x), it gives the correct output y.\nIn machine learning, however, we have a large set of inputs x and corresponding outputs y but not the function f(x). The goal here is to find the f(x) that transforms the input x into the output y. Well, that’s not an easy job. In this article, we will learn how this happens.\nDataset To visualize the dataset, let’s make our synthetic dataset where each data point (input x) is 3 dimensional, making it suitable to be plotted on a 3D chart. We will generate 250 points (cluster 0) in a cluster centered at the origin (0, 0, 0). A similar cluster of 250 points (cluster 1) is generated but not centered at the origin. Both clusters are relatively close but there is a clear separation as seen in the image below. These two clusters are the two classes of data points. The big green dot represents the centroid of the whole dataset.\nAfter generating the dataset, we will normalize it by subtracting the mean and dividing by the standard deviation. This is done to zero-center the data and map values in each dimension in the dataset to a common scale. This speeds up the learning.\nThe data will be saved in an array X containing the 3D coordinates of normalized points. We will also generate an array Y with the value either 0 or 1 at each index depending on which cluster the 3D point belongs.\nLearnable Function Now that we have our data ready, we can say that we have the x and y. We know that the dataset is linearly separable implying that there is a plane that can divide the dataset into the two clusters, but we don’t know what the equation of such an optimal plane is. For now, let’s just take a random plane.\nThe function f(x) should take a 3D coordinate as input and output a number between 0 and 1. If this number is less than 0.5, this point belongs to cluster 0 otherwise, it belongs to cluster 1. Let’s define a simple function for this task.\nx: input tensor of shape (num_points, 3)W: Weight (parameter) of shape (3, 1) chosen randomlyB: Bias (parameter) of shape (1, 1) chosen randomlySigmoid: A function that maps values between 0 and 1\nLet’s take a moment to understand what this function means. Before applying the sigmoid function, we are simply creating a linear mapping from the 3D coordinate (input) to 1D output. Therefore, this function will squish the whole 3D space onto a line meaning that each point in the original 3D space will now be lying somewhere on this line. Since this line will extend to infinity, we map it to [0, 1] using the Sigmoid function. As a result, for each given input, f(x) will output a value between 0 and 1.\nRemember that W and B are chosen randomly and so the 3D space will be squished onto a random line. The decision boundary for this transformation is the set of points that make f(x) = 0.5. Think why! As the 3D space is being squished onto a 1D line, a whole plane is mapped to the value 0.5 on the line. This plane is the decision boundary for f(x). Ideally, it should divide the dataset into two clusters but since W and B are randomly chosen, this plane is randomly oriented as shown below.\nOur goal is to find the right values for W and B that orients this plane (decision boundary) in such a way that it divides the dataset into the two clusters. This when done, yields a plane as shown below.\nLoss So, we are now at the starting point (random decision boundary) and we have defined the goal. We need a metric to decide how far we are from the goal. The output of the classifier is a tensor of shape (num_points, 1) where each value is between [0, 1]. If you think carefully, these values are just the probabilities of the points belonging to cluster 1. So, we can say that:\nf(x) = P(x belongs to cluster 1) 1-f(x) = P(x belongs to cluster 0) It wouldn’t be wrong to say that [1-f(x), f(x)] forms a probability distribution over the clusters 0 and cluster 1 respectively. This is the predicted probability distribution. We know for sure which cluster every point in the dataset belongs to (from y). So, we also have the true probability distribution as:\n[0, 1] when x belongs to the cluster 1 [1, 0] when x belongs to the cluster 0 A good metric to calculate the incongruity between two probability distributions is the Cross-Entropy function. As we are dealing with just 2 classes, we can use Binary Cross-Entropy (BCE). This function is available in PyTorch’s torch.nn module. If the predicted probability distribution is very similar to the true probability distribution, this function returns a small value and vice versa. We can average this value for all the data points and use it as a parameter to test how the classifier is performing.\nThis value is called the loss and mathematically, our goal now is to minimize this loss.\nTraining Now that we have defined our goal mathematically, how do we reach our goal practically? In other words, how do we find optimal values for W and B? To understand this, we will take a look at some basic calculus. Recall that we currently have random values for W and B. The process of learning or training or reaching the goal or minimizing the loss can be divided into two steps:\nForward-propagation: We feed the dataset through the classifier f(x) and use BCE to find the loss. Backpropagation: Using the loss, adjust the values of W and B to minimize the loss. The above two steps will be repeated over and over again until the loss stops decreasing. In this condition, we say that we have reached the goal!\nBackpropagation Forward propagation is simple and already discussed above. However, it is essential to take a moment to understand backpropagation as it is the key to machine learning. Recall that we have 3 parameters (variables) in W and 1 in B. So, in total, we have 4 values to optimize.\nOnce we have the loss from forward-propagation, we will calculate the gradients of the loss function with respect to each variable in the classifier. If we plot the loss for different values of each parameter, we can see that the loss is minimum at a particular value for each parameter. I have plotted the loss vs parameter for each parameter.\nAn important observation to make here is that the loss is minimized at a particular value for each of these parameters as shown by the red dot.\nLet’s consider the first plot and discuss how w1 will be optimized. The process remains the same for the other parameters. Initially, the values for W and B are chosen randomly and so (w1, loss) will be randomly placed on this curve as shown by the green dot.\nNow, the goal is to reach the red dot, starting from the green dot. In other words, we need to move downhill. Looking at the slope of the curve at the green dot, we can tell that increasing w1 (moving right) will lower the loss and therefore move the green dot closer to the red one. In mathematical terms, if the gradient of the loss with respect to w1 is negative, increase w1 to move downhill and vice versa. Therefore, w1 should be updated as:\nThe equation above is known as gradient descent equation. Here, the learning_rate controls how much we want to increase or decrease w1. If the learning_rate is large, the update will be large. This could lead to w1 going past the red dot and therefore missing the optimal value. If this value is too small, it will take forever for w1 to reach the red dot. You can try experimenting with different values of learning rate to see which works the best. In general, small values like 0.01 works well for most cases.\nIn most cases, a single update is not enough to optimize these parameters; so, the process of forward-propagation and backpropagation is repeated in a loop until the loss stops reducing further. Let’s see this in action:\nAn important observation to make is that initially the green dot moves quickly and slows down as it gradually approaches the minima. The large slope (gradient) during the first few epochs (when the green dot is far from the minima) is responsible for this large update to the parameters. The gradient decreases as the green dot approaches the minima and thus the update becomes slow. The other three parameters are trained in parallel in the exact same way. Another important observation is that the shape of the curve changes with epoch. This is due to the fact that the other three parameters (w2, w3, b) are also being updated in parallel and each parameter contributes to the shape of the loss curve.\nVisualize Let’s see how the decision boundary updates in real-time as the parameters are being updated.\nThat’s all folks! If you made it till here, hats off to you! In this article, we took a visual approach to understand how machine learning works. So far, we have seen how a simple 3D to 1D mapping, f(x), can be used to fit a decision boundary (2D plane) to a linearly separable dataset (3D). We discussed how forward propagation is used to calculate the loss followed by backpropagation where gradients of the loss with respect to parameters are calculated and the parameters are updated repeatedly in a training loop.\n","wordCount":"1672","inLanguage":"en","image":"https://awwais.me/playbook/blog/machine-learning-visualized/cover.jpeg","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Awais"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://awwais.me/playbook/blog/machine-learning-visualized/"},"publisher":{"@type":"Organization","name":"Awais Mustafa","logo":{"@type":"ImageObject","url":"https://awwais.me/playbook/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class="header sticky-header"><nav class=nav><div class=logo><a href=https://awwais.me/playbook accesskey=h title="Home (Alt + H)">Home</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://awwais.me/playbook/blog/ title=Blog><span>Blog</span></a></li><li><a href=https://awwais.me/playbook/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://awwais.me/playbook/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://awwais.me/playbook/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://awwais.me/playbook>Home</a>&nbsp;»&nbsp;<a href=https://awwais.me/playbook/blog/>Blogs</a></div><h1 class=post-title>Machine Learning - Visualized</h1><div class=post-description>A visual approach to understand machine learning</div><div class=post-meta>August 20208 min&nbsp;·&nbsp;1672 words&nbsp;·&nbsp;Awais&nbsp;|&nbsp;<a href=https://github.com/awwais/playbook/tree/master/content//blog/machine-learning-visualized.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>‎ Table of Contents</span></summary><div class=inner><nav id=TableOfContents></nav></div></details></div><div class=post-content><h1 id=introduction-to-machine-learning>Introduction to machine learning<a hidden class=anchor aria-hidden=true href=#introduction-to-machine-learning>#</a></h1><p>In the traditional hard-coded approach, we program a computer to perform a certain task. We tell it exactly what to do when it receives a certain input. In mathematical terms, this is like saying that we write the <em><strong>f(x)</strong></em> such that when users feed the input <em><strong>x</strong></em> into <em><strong>f(x)</strong></em>, it gives the correct output <em><strong>y</strong></em>.</p><p><img loading=lazy src=/blog/machine-learning-visualized/img1.jpg alt>
In machine learning, however, we have a large set of inputs x and corresponding outputs y but not the function f(x). The goal here is to find the f(x) that transforms the input x into the output y. Well, that’s not an easy job. In this article, we will learn how this happens.</p><h1 id=dataset>Dataset<a hidden class=anchor aria-hidden=true href=#dataset>#</a></h1><p>To visualize the dataset, let’s make our synthetic dataset where each data point (input <em><strong>x</strong></em>) is 3 dimensional, making it suitable to be plotted on a 3D chart. We will generate 250 points <strong>(cluster 0)</strong> in a cluster centered at the origin (0, 0, 0). A similar cluster of 250 points <strong>(cluster 1)</strong> is generated but not centered at the origin. Both clusters are relatively close but there is a clear separation as seen in the image below. These two clusters are the two classes of data points. The big green dot represents the centroid of the whole dataset.</p><p><img loading=lazy src=/blog/machine-learning-visualized/img2.jpg alt></p><p>After generating the dataset, we will normalize it by subtracting the mean and dividing by the standard deviation. This is done to zero-center the data and map values in each dimension in the dataset to a common scale. This speeds up the learning.</p><p><img loading=lazy src=/blog/machine-learning-visualized/img3.jpg alt></p><p>The data will be saved in an array X containing the 3D coordinates of normalized points. We will also generate an array Y with the value either 0 or 1 at each index depending on which cluster the 3D point belongs.</p><h1 id=learnable-function>Learnable Function<a hidden class=anchor aria-hidden=true href=#learnable-function>#</a></h1><p>Now that we have our data ready, we can say that we have the <em><strong>x</strong></em> and <em><strong>y.</strong></em> We know that the dataset is linearly separable implying that there is a plane that can divide the dataset into the two clusters, but we don’t know what the equation of such an optimal plane is. For now, let’s just take a random plane.</p><p>The function f(x) should take a 3D coordinate as input and output a number between 0 and 1. If this number is less than 0.5, this point belongs to cluster 0 otherwise, it belongs to cluster 1. Let’s define a simple function for this task.</p><p><img loading=lazy src=/blog/machine-learning-visualized/img4.jpg alt></p><p><em><strong>x</strong></em>: input tensor of shape (num_points, 3)W: Weight (parameter) of shape (3, 1) chosen randomlyB: Bias (parameter) of shape (1, 1) chosen randomlySigmoid: A function that maps values between 0 and 1</p><p>Let’s take a moment to understand what this function means. Before applying the sigmoid function, we are simply creating a linear mapping from the 3D coordinate (input) to 1D output. Therefore, <strong>this function will squish the whole 3D space onto a line</strong> meaning that each point in the original 3D space will now be lying somewhere on this line. Since this line will extend to infinity, we map it to <strong>[0, 1]</strong> using the <strong>Sigmoid</strong> function. As a result, for each given input, <em><strong>f(x)</strong></em> will output a value between 0 and 1.</p><p>Remember that W and B are chosen randomly and so the 3D space will be squished onto a random line. The decision boundary for this transformation is the set of points that make <strong><em>f(x)</em> = 0.5</strong>. Think why! As the 3D space is being squished onto a 1D line, a whole plane is mapped to the value 0.5 on the line. This plane is the decision boundary for f(x). Ideally, it should divide the dataset into two clusters but since <strong>W</strong> and <strong>B</strong> are randomly chosen, this plane is randomly oriented as shown below.</p><p><img loading=lazy src=/blog/machine-learning-visualized/img5.jpg alt></p><p>Our goal is to find the right values for W and B that orients this plane (decision boundary) in such a way that it divides the dataset into the two clusters. This when done, yields a plane as shown below.</p><p><img loading=lazy src=/blog/machine-learning-visualized/img6.jpg alt></p><h1 id=loss>Loss<a hidden class=anchor aria-hidden=true href=#loss>#</a></h1><p>So, we are now at the starting point (random decision boundary) and we have defined the goal. We need a metric to decide how far we are from the goal. The output of the classifier is a tensor of shape (num_points, 1) where each value is between <strong>[0, 1]</strong>. If you think carefully, these values are just the probabilities of the points belonging to cluster 1. So, we can say that:</p><ul><li>f(x) = P(x belongs to cluster 1)</li><li>1-f(x) = P(x belongs to cluster 0)</li></ul><p>It wouldn’t be wrong to say that [<em><strong>1-f(x), f(x)</strong></em>] forms a probability distribution over the clusters 0 and cluster 1 respectively. This is the <strong>predicted probability distribution</strong>. We know for sure which cluster every point in the dataset belongs to (from <em><strong>y</strong></em>). So, we also have the <strong>true probability distribution</strong> as:</p><ul><li>[0, 1] when x belongs to the cluster 1</li><li>[1, 0] when x belongs to the cluster 0</li></ul><p>A good metric to calculate the incongruity between two probability distributions is the <strong>Cross-Entropy</strong> function. As we are dealing with just 2 classes, we can use <strong>Binary Cross-Entropy (BCE).</strong> This function is available in PyTorch’s <strong>torch.nn</strong> module. If the predicted probability distribution is very similar to the true probability distribution, this function returns a small value and vice versa. We can average this value for all the data points and use it as a parameter to test how the classifier is performing.</p><p><img loading=lazy src=/blog/machine-learning-visualized/img7.jpg alt></p><p>This value is called the loss and mathematically, our goal now is to minimize this loss.</p><h1 id=training>Training<a hidden class=anchor aria-hidden=true href=#training>#</a></h1><p>Now that we have defined our goal mathematically, how do we reach our goal practically? In other words, how do we find optimal values for <strong>W</strong> and <strong>B</strong>? To understand this, we will take a look at some basic calculus. Recall that we currently have random values for <strong>W</strong> and <strong>B</strong>. The process of learning or training or reaching the goal or minimizing the loss can be divided into two steps:</p><ol><li><strong>Forward-propagation:</strong> We feed the dataset through the classifier <em><strong>f(x)</strong></em> and use <strong>BCE</strong> to find the <strong>loss.</strong></li><li><strong>Backpropagation:</strong> Using the loss, adjust the values of <strong>W</strong> and <strong>B</strong> to minimize the <strong>loss</strong>.</li></ol><p>The above two steps will be repeated over and over again until the loss stops decreasing. In this condition, we say that we have reached the goal!</p><h1 id=backpropagation>Backpropagation<a hidden class=anchor aria-hidden=true href=#backpropagation>#</a></h1><p>Forward propagation is simple and already discussed above. However, it is essential to take a moment to understand <strong>backpropagation</strong> as it is the key to machine learning. Recall that we have 3 parameters (variables) in <strong>W</strong> and 1 in <strong>B</strong>. So, in total, we have 4 values to optimize.</p><p><img loading=lazy src=/blog/machine-learning-visualized/img8.jpg alt></p><p>Once we have the loss from forward-propagation, we will calculate the gradients of the loss function with respect to each variable in the classifier. If we plot the loss for different values of each parameter, we can see that the loss is minimum at a particular value for each parameter. I have plotted the loss vs parameter for each parameter.</p><p><img loading=lazy src=/blog/machine-learning-visualized/img9.jpg alt></p><p>An important observation to make here is that the loss is minimized at a particular value for each of these parameters as shown by the red dot.</p><p>Let’s consider the first plot and discuss how w1 will be optimized. The process remains the same for the other parameters. Initially, the values for W and B are chosen randomly and so <strong>(w1, loss)</strong> will be randomly placed on this curve as shown by the green dot.</p><p><img loading=lazy src=/blog/machine-learning-visualized/img10.jpg alt></p><p>Now, the goal is to reach the red dot, starting from the green dot. In other words, we need to move downhill. Looking at the slope of the curve at the green dot, we can tell that increasing w1 (moving right) will lower the loss and therefore move the green dot closer to the red one. In mathematical terms, if the gradient of the loss with respect to w1 is negative, increase w1 to move downhill and vice versa. Therefore, w1 should be updated as:</p><p><img loading=lazy src=/blog/machine-learning-visualized/img11.jpg alt></p><p>The equation above is known as <strong>gradient descent equation</strong>. Here, the <strong>learning_rate</strong> controls how much we want to increase or decrease w1. If the learning_rate is large, the update will be large. This could lead to w1 going past the red dot and therefore missing the optimal value. If this value is too small, it will take forever for w1 to reach the red dot. You can try experimenting with different values of learning rate to see which works the best. In general, small values like <strong>0.01</strong> works well for most cases.</p><p>In most cases, a single update is not enough to optimize these parameters; so, the process of forward-propagation and backpropagation is repeated in a loop until the loss stops reducing further. Let’s see this in action:</p><p><img loading=lazy src=/blog/machine-learning-visualized/img12.gif#center alt></p><p>An important observation to make is that initially the green dot moves quickly and slows down as it gradually approaches the minima. The large slope (gradient) during the first few epochs (when the green dot is far from the minima) is responsible for this large update to the parameters. The gradient decreases as the green dot approaches the minima and thus the update becomes slow. The other three parameters are trained in parallel in the exact same way. Another important observation is that the shape of the curve changes with epoch. This is due to the fact that the other three parameters (w2, w3, b) are also being updated in parallel and each parameter contributes to the shape of the loss curve.</p><h1 id=visualize>Visualize<a hidden class=anchor aria-hidden=true href=#visualize>#</a></h1><p>Let’s see how the decision boundary updates in real-time as the parameters are being updated.</p><p><img loading=lazy src=/blog/machine-learning-visualized/img13.gif#center alt></p><h1 id=thats-all-folks>That’s all folks!<a hidden class=anchor aria-hidden=true href=#thats-all-folks>#</a></h1><p>If you made it till here, hats off to you! In this article, we took a visual approach to understand how machine learning works. So far, we have seen how a simple 3D to 1D mapping, <em><strong>f(x)</strong></em>, can be used to fit a decision boundary (2D plane) to a linearly separable dataset (3D). We discussed how forward propagation is used to calculate the loss followed by backpropagation where gradients of the loss with respect to parameters are calculated and the parameters are updated repeatedly in a training loop.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://awwais.me/playbook/tags/ml/>ML</a></li><li><a href=https://awwais.me/playbook/tags/ai/>AI</a></li><li><a href=https://awwais.me/playbook/tags/python/>Python</a></li><li><a href=https://awwais.me/playbook/tags/visualization/>Visualization</a></li></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Machine Learning - Visualized on twitter" href="https://twitter.com/intent/tweet/?text=Machine%20Learning%20-%20Visualized&url=https%3a%2f%2fawwais.me%2fplaybook%2fblog%2fmachine-learning-visualized%2f&hashtags=ML%2cAI%2cPython%2cVisualization"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Machine Learning - Visualized on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fawwais.me%2fplaybook%2fblog%2fmachine-learning-visualized%2f&title=Machine%20Learning%20-%20Visualized&summary=Machine%20Learning%20-%20Visualized&source=https%3a%2f%2fawwais.me%2fplaybook%2fblog%2fmachine-learning-visualized%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Machine Learning - Visualized on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fawwais.me%2fplaybook%2fblog%2fmachine-learning-visualized%2f&title=Machine%20Learning%20-%20Visualized"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Machine Learning - Visualized on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fawwais.me%2fplaybook%2fblog%2fmachine-learning-visualized%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Machine Learning - Visualized on whatsapp" href="https://api.whatsapp.com/send?text=Machine%20Learning%20-%20Visualized%20-%20https%3a%2f%2fawwais.me%2fplaybook%2fblog%2fmachine-learning-visualized%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Machine Learning - Visualized on telegram" href="https://telegram.me/share/url?text=Machine%20Learning%20-%20Visualized&url=https%3a%2f%2fawwais.me%2fplaybook%2fblog%2fmachine-learning-visualized%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://awwais.me/playbook>Awais Mustafa</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>